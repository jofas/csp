\section{Introduction}
\label{sec:intro}

Some datasets do not allow a classifier to generate a
decision surface good enough to be able to predict unseen
observations well. A dataset contains tuples of
observations and their labels. An observation is a point
inside the feature space whereas the label is an element
from a finite set called label set. A classification
problem is the goal to find a function that maps the
feature space to the label space based on the observations
and their respective labels provided by the dataset.%
\cite[chapter 18]{ki} This function is called a classifier.

Sometimes a classifier needs to fulfill a certain quality
criteria in order to consider it a solution to the
respective classification problem. For example: think of
any classification problem that needs a classifier to have
an accuracy of 99 percent---in this case impossible to
find for the given dataset.

For some of those classification problems it may still be
valuable to predict only on partitions of the feature
space if the problem is partially solvable. A
classification problem is partially solvable if a
classifier can return the label of an observation or
nothing if it is not certain enough it can predict the
correct label. Non-partial classifier always return a
label.

For the example above: if there exists at least one
partition of the feature space in which a classifier can be
found that has an accuracy of 99 percent the problem still
can be partially solved if this is desired.

More generally accuracy would be the quality metric,
the classifier's accuracy is the quality measurement and 99
percent is the quality threshold which must be met by a
classifier's quality measurement in order to consider the
classifier a (partial) solution to the classification
problem. Any metric---for example a loss metric---can be
used to determine a classifier's quality.

I call this concept Partial Classification. The idea
opposes the classical, non-partial approach. The
non-partial approach would be trying to increase a
classifier's quality until it equals or exceeds the quality
threshold. Partial Classification takes the other way
around. A partial classifier takes the quality threshold as
its input and tries to find partitions of the feature space
in which it can reach the quality. So rather than
increasing the quality of the classifier the goal in
Partial Classification is to increase the (sub-)space in
which a partial classifier can predict.

This paper proposes a Monte Carlo based ensemble method
realizing Partial Classification called Partial
Classification Forest (PCF). The PCF builds an ensemble of
trees having a structure similar to k-d trees. An instance
of this tree structure partitions the feature space of a
given dataset in order to find partitions, making Partial
Classification possible.

It should be noted here: this paper is rather a Proof
of Concept describing the structures and algorithms of a
very early version of the PCF. It has several shortcomings
in research and empirical tests, due to a lack of time and
no complete, fast implementation. These shortcomings are:

\begin{itemize}

\item Tests with a more sophisticated $\gamma$ (see
      Section~\ref{subsec:fit})

\item The PCF's performance on high dimensional data

\item Untested possible optimizations and features (see
      Section~\ref{sec:features})

\item A Tree's growing behavior

\item Benchmarks

\end{itemize}

In Section~\ref{sec:tree} I will lay out
the structure and the operations of the k-d tree like
structure the PCF uses.

In this paper such a tree generated by the PCF is
spelled Tree---with a capital T---rather than tree,
which is used to denote the tree data structure.

In Section~\ref{sec:pcf} I will describe how the PCF works
and utilizes Tree instances. After that I will continue
displaying the application of the PCF and compare it to
other, non-partial classifiers.

Because this paper is concerned with an early version of
the PCF, it will also contain a discussion about possible
additional features which could further increase the PCF's
performance in Section~\ref{sec:features}.

Last I will finish with my conclusions and a road map.
