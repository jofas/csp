\section{Introduction}

Some datasets do not allow a classifier to generate a
descision surface good enough to be able to predict unseen
observations well. Well, in this case, refers to a
context dependent threshold for any quality measurement of
a classifier, for example the accuracy or an information
loss metric.

But for some of those problems, it may still be valuable
to predict only on partitions of the feature space, in
which the dataset is `clean' enough, meaning a classifier
can be found within the subset of the dataset laying inside
one of those partitions which equals or exceeds the
threshold.

This paper proposes a Monte Carlo based ensemble method
called Partial Classification Forest (PCF), which builds an
ensemble of trees having a structure similar to
k-d trees to partition the feature space of the dataset in
order to find `clean' partitions. In the following a
tree generated by the PCF is spelled Tree with a capital
T, rather than tree, which is used to denote the tree data
structure.

In Section \MakeUppercase{\romannumeral 2} I will lay out
the structure and the operations of a Tree generated by the
PCF before, in Section \MakeUppercase{\romannumeral 3},
describing how PCF utilizes Tree instances. After that I
will continue displaying test results using PCF\@. In
Section \MakeUppercase{\romannumeral 5} I will discuss
further optimizations and possible additional features
before finishing with a conclusion.
