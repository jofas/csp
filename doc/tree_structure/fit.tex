\subsection{The FIT operation}
\label{subsec:fit}

The FIT operation constructs a Tree based on a dataset
split into observations ($X$) and their labels ($y$).
Algorithm~\ref{alg:fit} shows how FIT recursively builds a
Tree from a pointer to a Nil node.

The most important parameter passed to FIT is $\gamma$.
$\gamma$ is a function returning (\romannumeral 1) a
classifier and (\romannumeral 2) the loss of it. Otherwise
$\gamma$ is treated as a black box by the PCF, so what the
classifier is and how its loss is calculated are not
relevant to the PCF, as long as the classifier is callable%
\footnote{There could be another interface for the
  classifier, for example a predict method similar to the
  scikit-learn library.\cite{sklearn_api}}
and returns an element from the label set when being
called (Algorithm~\ref{alg:pred}, line 9). The loss
returned by $\gamma$ gets compared to the quality threshold
$\tau_l$. Is the loss $\leq \tau_l$ the classifier returned
by $\gamma$ is considered good enough and $\Theta$ is
transformed into an active Leaf (Algorithm~\ref{alg:fit},
lines 2, 3).

There are two other thresholds besides $\tau_l$:
$\tau_{|X|}$ and $\tau_h$. Both regulate the behavior of a
Tree's growth. $\tau_{|X|}$ defines a minimum amount of
observations a Leaf must contain. Without $\tau_{|X|}$ or
$\tau_{|X|} = 0$ a Tree would never stop growing, since FIT
would continue to split empty partitions trying to find a
smaller partition which could be predictable, even though
no classifier can be generated without observations to
train it on.

$\tau_h$ further regulates the maximum path length of a
Tree. It is necessary besides $\tau_{|X|}$: be
$\tau_{|X|} = 2$ and there are two equal observations in
the dataset, both having a different label than the other
one. $\gamma$---passed $X$ containing only those two
identical observations---returns a classifier with a
loss $> \tau_l$. Since $|X|$ is still not smaller than
$\tau_{|X|}$ FIT would continue trying to separate the two
inseparable observations. To prevent such a scenario
$\tau_h$ regulates FIT to stop before the Tree's height%
---the amount of edges of the longest path---would
exceed $\tau_h$. The path length of the Tree's root to
$\Theta$ is passed as a parameter $h$ to FIT. $\tau_h$ also
provides a way to further regulate the time complexity of
FIT and PREDICT.

FIT performs a split and transforms $\Theta$ to a Node
(Algorithm~\ref{alg:fit}, lines 7ff), if neither the
classifier's loss matches $\tau_l$ nor $\tau{|X|}$ or
$\tau_h$ is violated. The dimension the split is performed
on is chosen in a cyclic manner, a practice also applied to
k-d trees (Algorithm~\ref{alg:fit}, line 7).%
~\cite{Brown2015kdtree}
But rather than choosing the splitting value at the median
of the observations in the dimension---done in order to
construct balanced k-d trees---the splitting value
is determined randomly.\cite{Brown2015kdtree}

In order to chose a proper splitting value $\beta_X$ is
passed as another parameter to FIT. $\beta_X$ represents
the boundaries for every dimension of the feature space
based on $X$. For each dimension $\beta_X$ contains a
tuple with the minimum and maximum value in the dimension
of all observations in $X$.

$\beta_X[\text{dimension}]$ is passed to a pseudo-random
number generator\footnote{The implementation used in
  Section~\ref{sec:application} utilizes Python 3.6's
  random library.\cite[chapter 9.6]{python}}
generating a random value so that
$lower(\beta_X[\text{dimension}]) \leq \text{random number}
\leq upper(\beta_X[\text{dimension}])$
(Algorithm~\ref{alg:fit}, line 8).

Afterwards $X$, $y$, $\beta_X$ are split into two new
disjoint partitions and FIT is recursively applied to both
(Algorithm~\ref{alg:fit}, lines 10ff).

Since $\tau_h$ is defined, the maximum amount of nodes a
Tree can have is $2^{\tau_h + 1} - 1$, if the Tree is
perfectly balanced.\cite[chapter 16.1]{Teschl} For each
node FIT is called, so building a Tree has a worst case
time complexity of:
\begin{align}
  \mathcal{O}((2^{\tau_h + 1} -1)*\mathcal{O}(\text{FIT})).
\end{align}
$\mathcal{O}(\text{FIT})$ is determined by the size of $X$%
---since $X$ has to be iterated in order to split it---and
by $\mathcal{O}(\gamma)$. That said, a single FIT
operation would have a worst case time complexity of:
\begin{align}
  \mathcal{O}(|X| + \mathcal{O}(\gamma)),
\end{align} which means the time complexity of the whole
fitting process is:
\begin{align}
  \mathcal{O}((2^{\tau_h + 1} - 1) *
  (|X| + \mathcal{O}(\gamma)))
\label{eq:O_fit}
\end{align}

% algorithm {{{
\begin{algorithm}
  \caption{: FIT($\Theta, X, y, h, \beta_X, \gamma,
    \tau_{l}, \tau_{|X|}, \tau_{h}$)}%
  \label{alg:fit}
  A Tree's FIT operation.

  Inputs:

    \begin{tabu}{llX}
    $\Theta$ &$-$ &a pointer to a Nil node; initially
      pointing to the root node of an empty Tree,\\
    $X$ &$-$ &input data,\\
    $y$ &$-$ &labels of X,\\
    $h$ &$-$ &height of the Tree; initially $h = 0$,\\
    $\beta_X$ &$-$ &lower and upper boundaries of every
      dimension of X,\\
    $\gamma$ &$-$ &function returning a classifier and its
      loss,\\
    $\tau_{l}$ &$-$ &loss threshold,\\
    $\tau_{|X|}$ &$-$ &threshold for the size of X,\\
    $\tau_{h}$ &$-$ &height limit of the Tree
    \end{tabu}

  Output: void

  \noindent\rule{\linewidth}{0.4pt}

  \begin{algorithmic}[1]
    \STATE classifier, loss $\leftarrow \gamma(X, y)$
    \IF{loss $\leq \tau_{l}$}
      \STATE $\Theta \leftarrow$ LEAF(\TRUE, classifier,
         $X$, $y$)
    \ELSIF{$h > \tau_{h}$ \OR $|X| < \tau_{|X|}$ \OR
        loss $> \tau_{l}$}
      \STATE $\Theta \leftarrow$ LEAF(\FALSE, classifier,
        $X$, $y$)
    \ELSE
      \STATE dimension $\leftarrow h$ mod $|X[0]|$
      \STATE split $\leftarrow$ RANDOM($\beta_X[
        \text{dimension}]$)
      \STATE $\Theta \leftarrow$ NODE(split, NIL, NIL)
      \STATE split $X$, $y$ and $\beta_X$ into
        $X'$, $X''$, $y'$, $y''$, $\beta_X'$, $\beta_X''$
      \STATE FIT($\Theta$.left, $X'$, $y'$, $h + 1$,
        $\beta_X'$, \dots)
      \STATE FIT($\Theta$.right, $X''$, $y''$, $h + 1$,
        $\beta_X''$, \dots)
    \ENDIF
  \end{algorithmic}
\end{algorithm}
% }}}

\begin{figure*}
  \begin{subfigure}[b]{\textwidth}
    \centering
    \input{tree_structure/example_data}
    \caption{Scatterplot showing the observations and the
      splits done by the FIT operation.}
    \label{fig:scatter_example}
  \end{subfigure}
  \begin{subfigure}[b]{\textwidth}
    \centering
    \scalebox{0.9}{\input{tree_structure/example_tree}}
    \caption{The structure of the Tree generated by FIT.}
    \label{fig:tree_example}
  \end{subfigure}
  \caption{Example of FIT applied to the dataset seen in
    Figure~\ref{fig:scatter_example}. $\gamma$ simply
    computes the probability of each label in $y$ and
    returns a function returning the label with the
    maximum probability and as loss one minus the maximum
    probability. The thresholds are: $\tau_l = 1$,
    $\tau_{|X|} = 2$. $\tau_h$ can be any integer above
    2.
  }
  \label{fig:fit_example}
\end{figure*}
