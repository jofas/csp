\section{Application}
\label{sec:application}

This Section will further outline the use case of Partial
Classification and the PCF, displaying results of a test on
an artificial dataset and comparing the PCF to other,
non-partial classifiers. Furthermore a test of the
PCF's behavior with different amounts of Trees will be
shown.

Both tests are performed on a randomly generated,
normalized, two dimensional and binary labeled dataset. The
dataset contains five thousand observations and was
designed to be unpredictable, when predicted as a whole.
The plane from which the observations are generated
contains five partitions in which the observations all have
the same label. Observations from those five partitions
make up twenty percent of all observations and are the only
ones which should be predicted, because all points not
inside those partitions are labeled randomly. The optimal
decision surface would be equal to the area of the five
partitions.

All observations are generated by a pseudo-random number
generator\cite[chapter 9.6]{python}, therefore: every point
on the plane has the same probability to be chosen as an
observation for the dataset.

The dataset is designed this way in order to be able to
show the application of the PCF in the domain of Partial
Classification. Only twenty percent of the whole dataset is
predictable which makes it impossible for other,
non-partial classifiers to be able to find an adequate
decision surface (see Figure~\ref{fig:other_classif}). The
following tests all take $\tau_l = 1$ as quality threshold
which is impossible to achieve predicting on the whole
dataset.

It is common practice in machine learning to split a
dataset into a training and a test set in order to find
the best model.\cite[chapter 18]{ki}
This approach is also used for the PCF. The training set is
used as the parameters $X$ and $y$ of FIT (Algorithm~%
\ref{alg:pcf_fit}) while PREDICT (Algorithm~%
\ref{alg:pcf_pred}) is used on every observation of the
test set.

Two metrics are used to describe the behavior of the PCF,
(\romannumeral 1) $predicted$ and (\romannumeral 2)
$accuracy$. Both metrics are derived from comparing the
label of an observation returned by PREDICT with its actual
label from the dataset.

$Predicted$ is the percentage of predicted observations,
while $accuracy$ is the percentage of correctly classified
observations from the test set. $Accuracy$ is used as the
quality measurement, which means in the context of this
tests, every classifier not able to achieve $accuracy = 1$
when predicting the test set does not solve the
classification problem.

For both tests $\gamma$, $\tau_l$ and $\tau_h$ are the
same. $\gamma$ and $\tau_l$ are equal to the values used in
Figure~\ref{fig:fit_example} while $\tau_h$ equals 32. The
dataset was split into a training and a test set such
that ten percent of the observations were used as the test
set.\footnote{For splitting scikit-learn's
  model\_selection.train\_test\_split was used during the
  tests.\cite{sklearn_api}} The observations for the test
set were chosen randomly.

The first test will show the decision surfaces of PCF
instances with different $N$ and $\tau_{|X|}$. For the test
three different values, two, five and ten were used for
both $N$ and $\tau_{|X|}$ to show how those two parameters
change the decision surface of the PCF instances. The test
shows that for $\tau_{|X|} = 2$ the PCF instances
over-fitted the data, $predicted$ exceeding twenty percent
--- the amount of accurate observations --- while
$accuracy$ fails to equal $\tau_l$. This results in the
chaotic decision surfaces shown in Figures~\ref{fig:x2_n2}
- \ref{fig:x2_n10}. On the other hand for $\tau{|X|} = 10$
the decision surface is very small which means the PCF
instances' $predicted$ is far less than twenty percent
(Figures~\ref{fig:x10_n2} - \ref{fig:x10_n10}).

How non-partial classifiers perform on the dataset can be
seen in Figure~\ref{fig:other_classif}. Three common
classifiers are tested, all from the scikit-learn library
(version 0.20.1).\cite{sklearn_api}
Their parameters are the standard parameters given by
scikit-learn. Tested were the Support Vector Machine,
Random Forest and K Nearest Neighbors, where K in this case
equals five.\cite{sklearn_api}
Figure~\ref{fig:other_classif} also shows the $accuracy$ of
these classifiers. None are close to $\tau_l = 1$.

The second test shows the influence the amount of Trees $N$
has on $predicted$ and $accuracy$. In this test
$\tau_{|X|}$ equals four.

Figure~\ref{fig:n} shows: for this dataset with the
chosen thresholds the PCF instances with $N < 30$ variate,
both their $predicted$ and $accuracy$ values. Is $30 \leq N
\leq 100$ $accuracy$ is constant and equals $\tau_l$.
$Predicted$ on the other hand still rises in this
interval. Is $N \geq 100$ $predicted$ is higher than
$predicted$ with $N = 100$ and constant, but $accuracy$
fails to meet $\tau_l$, which means the PCF instances are
over-fitting.

Furthermore the second test relates $predicted$ and
$accuracy$ of the observations from the test set to the
values for the training set, showing that more training
observations are predictable than test observations
(see Figure~\ref{fig:n}).

\input{application/decision_surface}

\input{application/n_estimators}
