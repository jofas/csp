\section{Introduction}

Some datasets do not allow a classifier to generate a
descision surface good enough to be able to predict unseen
observations well. A dataset contains tuples of
observations and their labels. An observation is a point
inside the feature space whereas the label is an element
from a finite set called label set. A classification
problem is the goal to find a function that maps the
feature space to the label space based on the observations
and their respective labels provided by the dataset.%
\cite[chapter 18]{ki} This function is called a classifier.

Sometimes a classifier needs to fulfill a certain quality
criteria in order to consider it a solution to the
respective classification problem. For example think of any
classification problem that needs a classifier to have an
accuracy of 99 percent, which is impossible to find for a
given dataset.

For some of those classification problems it may still be
valuable to predict only on partitions of the feature
space if the problem is partially solvable. A problem is
partially solvable if a classifier should return the label
of an observation as the desired output, but can also
return nothing if the classifier is ``not shure'' enough
whether it even can return the correct label.

I call this whole concept Partial Classification.

For the example above: if there exists at least one
partiton of the feature space in which a classifier can be
found that has an accuracy of 99 percent the problem still
can be partially solved if this is desired.

More generally accuracy would be the quality metric,
the classifier's accuracy is the quality measurement and 99
percent the quality theshold which must be met by a
classifier's quality measurement in order to consider the
classifier a (partial) solution to the classification
problem. Any metric, for example a loss metric, can be used
to determine a classifier's quality.

This paper proposes a Monte Carlo based ensemble method
called Partial Classification Forest (PCF), which builds an
ensemble of trees having a structure similar to
k-d trees to partition the feature space of a given dataset
in order to find partitions making Partial Classification
possible.

It should be noted here, that this paper is rather a Proof
of Concept describing the structures and algorithms of a
very early version of the PCF. It has several shortcomings
in research and empirical tests, due to a lack of time and
no complete, fast implementation. I will discuss these
shortcomings further in Chapter~\ref{sec:conclusions}:

\begin{itemize}

\item Tests with a more sofisticated $\gamma$ (cmp.
      Section~\ref{subsec:fit})

\item The PCF's performance on high dimensional data

\item Untested possible optimizations and features (cmp.
      Section~\ref{sec:features})

\item A Tree's growing behaviour

\item Benchmarks

\end{itemize}

In Section~\ref{sec:tree} I will lay out
the structure and the operations of the k-d tree like
structure the PCF uses.

In the following text such a tree generated by the PCF is
spelled Tree with a capital T, rather than tree, which is
used to denote the tree data structure.

In Section~\ref{sec:pcf} I will describe how the PCF works
and utilizes Tree instances. After that I will continue
displaying the application of the PCF and compare it to
other, non-partial classifiers.

Because this paper is concerned with an early version of
the PCF it will also contain a discussion about possible
additional features in Section~\ref{sec:features}.

After that I will finish with my conclusions.
