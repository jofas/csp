\section{Conclusions}
\label{sec:conclusions}

In Section~\ref{sec:intro} I described what I mean by
Partial Classification. The whole concept is based on the
condition that a classifier needs to maintain a certain
quality in its predictions for a given classification
problem. If it can not do this, it is not regarded a
solution.

Partial Classification is a way to still find classifiers
that partially solve a classification problem not solvable
as a whole. A partial classifier only predicts the label
of an observation if it lays inside a partition it can
classify. Otherwise it returns nothing, indicating that
the classifier is not certain which label the observation
has.

This changes the whole approach to increasing a
classifier's quality. Since a partial classifier has its
quality set beforehand, the goal to increasing the partial
classifier further is to enlarge the (sub-)space it can
predict on. On the other hand: non-partial classifier do
not have the quality given as a threshold, which means
their quality is variable rather than the space they
predict on.

This paper provides a description of a Partial
Classification method, the Partial Classification Forest
(PCF). The PCF's biggest advantage is the fact it is a very
flexible Meta-Classifier, since it treats the classifier
instances it relies on (and their quality measurement) as a
black box (see Section~\ref{subsec:fit} - $\gamma$).

This paper describes the core structures and operations of
the PCF and shows its use on an artificial problem designed
to display the use of it over other, non-partial
classifiers.

Also stated in the Introduction, this paper has some
shortcomings in research and empirical tests regarding the
PCF, due to a lack of time and no complete, fast
implementation. A list of these shortcomings is also
provided in the Introduction. The next step will be to make
a fast and complete implementation before proceeding with
the research and the empirical tests.
