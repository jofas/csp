\documentclass[journal]{IEEEtran}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[colorlinks=true, linkcolor=black]{hyperref} % Links
\usepackage{makeidx} % Indexierung
\usepackage{siunitx}
%\usepackage[ngerman]{babel} % deutsche Sonderzeichen
\usepackage[utf8]{inputenc}
\usepackage{geometry} % Dokumentendesign wie Seiten- oder Zeilenabstand bestimmen
\usepackage{algorithm}
\usepackage{algorithmic}
%\usepackage[toc,page]{appendix}

% Graphiken
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{pgfcore}
\usepackage{pgfopts}
\usepackage{pgfornament}
\usepackage{pgf}
\usepackage{ifthen}
\usepackage{booktabs}

% Tabellen
\usepackage{tabu}
\usepackage{longtable}
\usepackage{colortbl} % Tabellen faerben
\usepackage{multirow}
\usepackage{diagbox} % Tabellenzelle diagonal splitten

\usepackage{xcolor} % Farben
\usepackage[framemethod=tikz]{mdframed} % Hintergrunderstellung
\usepackage{enumitem} % Enumerate mit Buchstaben nummerierbar machen
\usepackage{pdfpages}
\usepackage{listings} % Source-Code darstellen
\usepackage{eurosym} % Eurosymbol
\usepackage[square,numbers]{natbib}
\usepackage{here} % figure an richtiger Stelle positionieren
\usepackage{verbatim} % Blockkommentare mit \begin{comment}...\end{comment}
\usepackage{ulem} % \sout{} (durchgestrichener Text)
\usepackage{abstract}
\usepackage{blindtext}
\usepackage{fancyref}

% BibLaTex
\bibliographystyle{acm}

% Aendern des Anhangnamens (Seite und Inhaltsverzeichnis)
%\renewcommand\appendixtocname{Anhang}
%\renewcommand\appendixpagename{Anhang}

% mdframed Style {{{
\mdfdefinestyle{codebox}{
	linewidth=2.5pt,
	linecolor=codebordercolor,
	backgroundcolor=codecolor,
	shadow=true,
	shadowcolor=black!40!white,
	fontcolor=black,
	everyline=true,
}
% }}}

% Seitenabstaende
%\geometry{left=15mm,right=15mm,top=15mm,bottom=20mm}

% TikZ Bibliotheken {{{
\usetikzlibrary{
    arrows,
    arrows.meta,
    decorations,
    backgrounds,
    positioning,
    fit,
    petri,
    shadows,
    datavisualization.formats.functions,
    calc,
    shapes,
    shapes.multipart
}
% }}}

\pgfplotsset{width=7cm,compat=1.15}

\definecolor{codecolor}{HTML}{EEEEEE}
\definecolor{codebordercolor}{HTML}{CCCCCC}

% Standardeinstellungen fuer Source-Code listings {{{
\lstset{
    language=C,
    breaklines=true,
    keepspaces=true,
    keywordstyle=\bfseries\color{green!70!black},
    basicstyle=\ttfamily\color{black},
    commentstyle=\itshape\color{purple},
    identifierstyle=\color{blue},
    stringstyle=\color{orange},
    showstringspaces=false,
    rulecolor=\color{black},
    tabsize=2,
    escapeinside={\%*}{*\%},
}
% }}}

%\input{libuml}
%\input{liberm}

\title{Partial Classification Forest}

\author{Jonas Fa{\ss}bender \\ [1ex]
  \href{mailto: jonas@fc-web.de}
  {jonas@fc-web.de}}
\date{}

\begin{document}

%\tableofcontents
%\newpage
%\listoffigures

\maketitle

\begin{abstract}
  %\noindent  \blindtext
\end{abstract}

\section{Introduction}

Some datasets do not allow a classifier to generate a
descision surface good enough to be able to predict unseen
observations well enough. Well, in this case, referrs to a
context dependent threshold for any quality measurement of
a classifier, for example the accuracy or an information
loss metric.

But for some of those problems, it may still be valuable
to predict only on partitions of the feature space, in
which the dataset is `clean' enough, meaning a classifier
can be found within the subset of the dataset laying inside
one of those partitions which equals or exceeds the
threshold.

This paper proposes a Monte Carlo based ensemble method
called Partial Classification Forest (PCF), builing an
ensemble of trees having a structure similar to
k-d trees to partition the feature space of the dataset in
order to find `clean' partitions. In the following a
tree generated by the PCF is spelled Tree with a capital
T, rather than tree, which is used to denote the tree data
structure.

In Section \MakeUppercase{\romannumeral 2} I will lay out
the structure of a Tree generated by the PCF before, in
Section \MakeUppercase{\romannumeral 3}, describing how
PCF uses the Trees and listing its parameters. After that I
will continue displaying test results using PCF\@. In
Section \MakeUppercase{\romannumeral 5} I will discuss
further optimizations and possible additional features
before finishing with a conclusion.

\section{The Tree structure}

A Tree generated by the PCF is a binary search tree
structure similar to k-d trees. Its purpose is to randomly
generate disjoint partitions of a feature space.

A Tree has two types of nodes, non-leaf nodes, here denoted
as Nodes and leaf nodes denoted as Leafs. It provides two
operations: (\romannumeral 1) FIT, initializing the Tree
and (\romannumeral 2) PREDICT, returning a label for an
oberservation.

The Node structure contains three properties:
(\romannumeral 1) a split value; (\romannumeral 2) a left
and (\romannumeral 3) a right successor, both references to
either another Node or a Leaf.

A Leaf on the other hand, is the structure
representing a partition of the feature space, having
(\romannumeral 1) active, a boolean value deciding whether
the partition's quality, determined during the FIT
operation, is equal or better than the defined threshold or
not; (\romannumeral 2) a predictor which is used to
classify oberservation during the PREDICT operation and two
vectors with arbitrary length (\romannumeral 3) containing
the observations of the dataset used in FIT, which are
laying inside the partition and (\romannumeral 4) their
inherent labels.

During the FIT operation a Tree contains a third type of
node, Nil. Nil is used to initialize Trees
and the left and right successor of a Node. These nodes are
transformed during FIT to either a Node or a Leaf, so after
the FIT operation a Tree does not contain Nil nodes
anymore.

%TODO: \Lambda as not a label, describe FIT and PREDICT

\begin{algorithm}
  \caption{: FIT($\Theta, X, y, h, \beta_X, \gamma,
    \tau_{g}, \tau_{|X|}, \tau_{h}$)}%
  \label{alg:fit}
  Inputs:

    \begin{tabu}{llX}
    $\Theta$ &$-$ &a pointer to a Nil node; initially
      pointing to the root node of an empty Tree,\\
    $X$ &$-$ &input data,\\
    $y$ &$-$ &labels of X,\\
    $h$ &$-$ &height of the Tree; initially $h = 0$,\\
    $\beta_X$ &$-$ &lower and upper boundries of every
      dimension of X,\\
    $\gamma$ &$-$ &function returning a predictor an its
      quality,\\
    $\tau_{g}$ &$-$ &quality threshold,\\
    $\tau_{|X|}$ &$-$ &threshold for the size of X,\\
    $\tau_{h}$ &$-$ &height limit of the Tree
    \end{tabu}

  Output: void

  \noindent\rule{\linewidth}{0.4pt}

  \begin{algorithmic}[1]
    \STATE predictor, gain $\leftarrow \gamma(X, y)$
    \IF{$h > \tau_{h}$ \OR $|X| < \tau_{|X|}$ \OR
        gain $< \tau_{g}$}
      \STATE $\Theta \leftarrow$ LEAF(\FALSE, predictor,
        $X$, $y$)
    \ELSIF{gain $\geq \tau_{g}$}
      \STATE $\Theta \leftarrow$ LEAF(\TRUE, predictor,
         $X$, $y$)
    \ELSE
      \STATE dimension $\leftarrow h$ mod $|X[0]|$
      \STATE split $\leftarrow$ RANDOM($\beta_X[$dimension
        $]$)
      \STATE $\Theta \leftarrow$ NODE(split, NIL, NIL)
      \STATE split $X$, $y$ and $\beta_X$ into
        $X'$, $X''$, $y'$, $y''$, $\beta_X'$, $\beta_X''$
      \STATE FIT($\Theta$.left, $X'$, $y'$, $h + 1$,
        $\beta_X'$, \dots)
      \STATE FIT($\Theta$.right, $X''$, $y''$, $h + 1$,
        $\beta_X''$, \dots)
    \ENDIF
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}
  \caption{: PREDICT($\Theta, x, h$)}%
  \label{alg:pred}
  Inputs:

    \begin{tabu}{llX}
    $\Theta$ &$-$ &a Tree node; initially pointing to the
      root of the Tree,\\
    $x$ &$-$ &an observation,\\
    $h$ &$-$ &height of the Tree; initially $h = 0$
    \end{tabu}

  Output: the predicted label or $\Lambda$

  \noindent\rule{\linewidth}{0.4pt}

  \begin{algorithmic}[1]
    \IF{TYPE($\Theta$) is Node}
      \STATE dimension $\leftarrow h$ mod $|x|$
      \IF{$x[dimension] \leq \Theta$.split}
        \STATE PREDICT($\Theta$.left, $x$, $h + 1$)
      \ELSE
        \STATE PREDICT($\Theta$.right, $x$, $h + 1$)
      \ENDIF
    \ELSIF{$\Theta$.active}
      \RETURN $\Theta$.predictor($x$)
    \ELSE
      \RETURN $\Lambda$
    \ENDIF
  \end{algorithmic}
\end{algorithm}

% TODO: find paper on k-d trees
% picuture of a 2d dataset scatter plot (matplotlib) with
% lines and a graph of the tree
%
% after the picture describe the fitting and predicting of
% of the Tree

% 4: tests
% 5: discussion
% 6: conclusion

%For some datasets the quality of the data is not high
%enough to find a classifier

%!!! https://www.kaggle.com/zynicide/wine-reviews

%Notes: - compare workflow against anomaly detection
%         (outlier removal) + classifier vs. PCF
%
%       - find real datasets for testing (3?)
%
%       - read IF and other papers describing algorithms
%
%       - read How to write a good Scientific Paper


%\section{Partial Classification Forest}

%\section{Partial Classification using PCF}

%\section{Comparison to Anomaly Detection Algorithms}

%\section{Discussion}

%\section{Conclusion}

\end{document}
